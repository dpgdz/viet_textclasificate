{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/duongphuonggiang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import wandb\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from datasets import load_dataset\n",
    "\n",
    "import dgl\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.nn.pytorch import GraphConv, GATConv, GatedGraphConv, DotGatConv\n",
    "from dgl.nn import AvgPooling, MaxPooling\n",
    "\n",
    "from underthesea import text_normalize\n",
    "from nltk.corpus import words as eng_word\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uit = load_dataset(\"uitnlp/vietnamese_students_feedback\")\n",
    "uit_train=uit[\"train\"]\n",
    "uit_test=uit[\"test\"]\n",
    "train=uit_train.to_pandas()\n",
    "test=uit_test.to_pandas\n",
    "#i use cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1959/1959 [00:00<00:00, 1670049.09it/s]\n",
      "100%|██████████| 9635/9635 [00:00<00:00, 2796105.93it/s]\n"
     ]
    }
   ],
   "source": [
    "teencode=dict()\n",
    "\n",
    "with codecs.open(\"teencode.txt\",\"r\",\"utf-8\") as f:\n",
    "    tc_temp=f.readlines()\n",
    "\n",
    "for line in tc_temp:\n",
    "    line=line.replace(\"\\t\",\" \").replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "    line_w=line.split()\n",
    "    # key=line_w[0].replace(\"_\",\" \")\n",
    "    teencode[line_w[0]]=\" \".join(line_w[1:])\n",
    "\n",
    "with open(\"vn_stopword.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    stopword=f.readlines()\n",
    "for i in tqdm(range(len(stopword))):\n",
    "    stopword[i]=stopword[i][:-1]\n",
    "    stopword[i]=stopword[i].replace(\" \",\"_\")\n",
    "stopword=set(stopword)\n",
    "\n",
    "with open(\"common_eng.txt\",\"r\") as f:\n",
    "    eng=f.readlines()\n",
    "    for i in tqdm(range(len(eng))):\n",
    "        eng[i]=eng[i][:-1]\n",
    "eng_common=set(eng)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, eng_common, teencode, stopword):\n",
    "        self.eng_common = eng_common\n",
    "        self.teencode = teencode\n",
    "        self.stopword = stopword\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.text = \"\"\n",
    "\n",
    "    def non_keo_dai(self):\n",
    "        temp = self.text.split()\n",
    "        for i in range(len(temp)):\n",
    "            new_text = \"\"\n",
    "            if temp[i] in self.eng_common:\n",
    "                temp[i] = self.lemmatizer.lemmatize(temp[i])\n",
    "                continue\n",
    "            if \"http\" in self.text:\n",
    "                continue\n",
    "            for c in temp[i]:\n",
    "                if len(new_text) < 1:\n",
    "                    new_text += c\n",
    "                elif c != new_text[-1] or c.isnumeric():\n",
    "                    new_text += c\n",
    "            temp[i] = new_text\n",
    "        self.text = \" \".join(temp)\n",
    "\n",
    "    def find_link(self):\n",
    "        regex = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "        matches = re.findall(regex, self.text)\n",
    "        for match in matches:\n",
    "            self.text = self.text.replace(match, \"\")\n",
    "\n",
    "    def keep_text_only(self):\n",
    "        newtext = \"\"\n",
    "        for c in self.text:\n",
    "            if c.isalpha() or c in [\".\", \" \"] or c in \"áàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ\":\n",
    "                newtext += c\n",
    "            else:\n",
    "                newtext += \" \"\n",
    "        self.text = newtext\n",
    "\n",
    "    def remove_tag_emoji_nl(self):\n",
    "        self.text = self.text.replace(\"<TAG>\", \" \")\n",
    "        self.text = self.text.replace(\"<EMOJI>\", \" \")\n",
    "        self.text = self.text.replace(\"<Emoji>\", \" \")\n",
    "        self.text = self.text.replace(\"\\n\", \".\")\n",
    "        self.text = self.text.replace(\"/\", \" \")\n",
    "\n",
    "    def checkspam(self):\n",
    "        newtext = \"\"\n",
    "        sen = self.text.split(\".\")\n",
    "        for se in sen:\n",
    "            if len(se.strip()) <= 2:\n",
    "                newtext += se\n",
    "            else:\n",
    "                newtext += se + \".\"\n",
    "        self.text = newtext\n",
    "\n",
    "    def rmv_teencode(self):\n",
    "        words = self.text.split()\n",
    "        # 2 words\n",
    "        for i in range(1, len(words)):\n",
    "            if words[i - 1] + \"_\" + words[i] in self.teencode:\n",
    "                temp = self.teencode[words[i - 1] + \"_\" + words[i]].split(\" \")\n",
    "                if len(temp) <= 1:\n",
    "                    words[i - 1] = temp[0]\n",
    "                    words[i] = \" \"\n",
    "                    continue\n",
    "                words[i] = \" \".join(temp[1:])\n",
    "\n",
    "        # 1 word\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in self.teencode:\n",
    "                words[i] = self.teencode[words[i]]\n",
    "        self.text = \" \".join(words)\n",
    "\n",
    "    def rm_vn_stop_word(self):\n",
    "        words = self.text.split()\n",
    "        # 2 words\n",
    "        for i in range(1, len(words)):\n",
    "            if words[i - 1] + \"_\" + words[i] in self.stopword:\n",
    "                words[i - 1] = \"\"\n",
    "                words[i] = \"\"\n",
    "\n",
    "        # 1 word\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in self.stopword:\n",
    "                words[i] = \"\"\n",
    "        self.text = \" \".join(words)\n",
    "\n",
    "    def refine_sentence(self):\n",
    "        self.rmv_teencode()\n",
    "        word = False\n",
    "        self.rm_vn_stop_word()\n",
    "        self.text = \" \".join(self.text.split())\n",
    "        self.rmv_teencode()\n",
    "        self.keep_text_only()\n",
    "        self.text = \" \".join(self.text.split())\n",
    "        if len(self.text) <= 1:\n",
    "            return \"\"\n",
    "\n",
    "        for t in self.text:\n",
    "            if t not in \"0123456789 .\":\n",
    "                word = True\n",
    "                break\n",
    "        if word:\n",
    "            return self.text.strip()\n",
    "        return \"\"\n",
    "\n",
    "    def runall(self):\n",
    "        self.non_keo_dai()\n",
    "        self.find_link()\n",
    "        self.keep_text_only()\n",
    "        self.remove_tag_emoji_nl()\n",
    "        self.checkspam()\n",
    "        self.rmv_teencode()\n",
    "        self.rm_vn_stop_word()\n",
    "        return self.refine_sentence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11426/11426 [00:02<00:00, 5149.19it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean(data):\n",
    "    processor = TextProcessor(eng_common, teencode, stopword)\n",
    "    cleaned_sentences = []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        sentence = data[i]\n",
    "        processor.text = sentence\n",
    "        cleaned_sentence = processor.runall()\n",
    "        cleaned_sentence=ViTokenizer.tokenize(cleaned_sentence)\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "    data.drop(columns=['sentence'], inplace=True)\n",
    "    data = cleaned_sentences\n",
    "    return data\n",
    "\n",
    "train[\"sentence\"]=clean(train[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       slide giáo_trình .\n",
       "1                     nhiệt_tình giảng_dạy gũi sinh_viên .\n",
       "2                                     đi học full chuyên .\n",
       "3                áp_dụng công_nghệ thông_thiết giảng_dạy .\n",
       "4                               thầy giảng tập ví_dụ lớp .\n",
       "                               ...                        \n",
       "11421              môn game học hai hài vô chuyên_nghiệp .\n",
       "11422                                                     \n",
       "11423                                           giao tập .\n",
       "11424                           giáo_viên dạy nhiệt_tình .\n",
       "11425    gói gọn doubledot tận_tình trình_độ nhu_cầu_mô...\n",
       "Name: sentence, Length: 11426, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"sentence\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1587508/1587508 [03:01<00:00, 8727.50it/s] \n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(path):\n",
    "    with open(path,'rb') as f:\n",
    "        emb=pickle.load(f)\n",
    "    return emb\n",
    "\n",
    "def check_coverage (vocab, embeddings_index):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    a={}\n",
    "    oov={}\n",
    "    k=0\n",
    "    i=0\n",
    "    \n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word]=embeddings_index[word]\n",
    "            k+=vocab[word]\n",
    "        except:\n",
    "            oov[word]=vocab[word]\n",
    "            i+=vocab[word]\n",
    "            pass\n",
    "    print(\"Found embeddings for {:.2%} of vocab\".format(len(a)/len(vocab)))\n",
    "    print (\"Found embeddings for {:.2%} of all text\".format(k/(k+i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_x\n",
    "\n",
    "def build_vocab(sentences, verbose=True):\n",
    "    vocab={}\n",
    "    for sentence in tqdm(sentences,disable=(not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word]+=1\n",
    "            except KeyError:\n",
    "                vocab[word]=1\n",
    "    return vocab\n",
    "\n",
    "glove_path=\"word2vec_vi_words_100dims.txt\"\n",
    "\n",
    "word_embeddings={}\n",
    "\n",
    "with open(glove_path,\"r\") as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        data=line.split()\n",
    "        word_embeddings[\" \".join(data[:-100])]=list(map(float, data[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11426/11426 [00:00<00:00, 647955.94it/s]\n",
      "100%|██████████| 2735/2735 [00:00<00:00, 113911.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 85.41% of vocab\n",
      "Found embeddings for 97.80% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('wzjwz', 236),\n",
       " ('doubledot', 87),\n",
       " ('hòa', 71),\n",
       " ('học_sinh_viên', 53),\n",
       " ('trang_thiết', 41),\n",
       " ('minh_họa', 38),\n",
       " ('khóa', 33),\n",
       " ('truyền_cảm_hứng', 30),\n",
       " ('tận_tụy', 30),\n",
       " ('thực_hành_lý_thuyết', 28)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check coverage\n",
    "vocab = build_vocab([sentence.split() for sentence in train[\"sentence\"]])\n",
    "oov = check_coverage(vocab, word_embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentence=train[\"sentence\"].apply(lambda x:\" \".join(x.split(\"_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check coverage\n",
    "# vocab = build_vocab([sentence.split() for sentence in train_sentence])\n",
    "# oov = check_coverage(vocab, word_embeddings)\n",
    "# oov[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Args for building graphs and training\n",
    "\n",
    "class args:\n",
    "    epochs=30\n",
    "    lr=1e-3\n",
    "    batch_size=64\n",
    "    embedding_dim=100\n",
    "    n_folds=5\n",
    "    window_size=3\n",
    "    num_heads=8 \n",
    "    hidden_dim=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_dim=args.embedding_dim\n",
    "\n",
    "shuffle_doc_words_list=list(train[\"sentence\"].values)+list(train[\"sentence\"].values)\n",
    "word_set=set()\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words=doc_words.split()\n",
    "    word_set.update(words)\n",
    "\n",
    "vocab=list(word_set)\n",
    "vocab_size=len(vocab)\n",
    "\n",
    "word_id_map={}\n",
    "\n",
    "for i in range (vocab_size):\n",
    "    word_id_map[vocab[i]]=i\n",
    "\n",
    "oov={}\n",
    "\n",
    "for v in vocab:\n",
    "    oov[v] =np.random.uniform(-0.1 , 0.1, word_embeddings_dim)\n",
    "\n",
    "window_size=args.window_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph (start, end, truncate =False, weighted_graph=True):\n",
    "\n",
    "    x_adj=[]\n",
    "    x_feature=[]\n",
    "    doc_len_list=[]\n",
    "    vocab_set=set()\n",
    "\n",
    "    for i in tqdm(range(start, end)):\n",
    "        doc_words=shuffle_doc_words_list[i].split()\n",
    "        if truncate:\n",
    "            doc_words=doc_words[:MAX_TRUNG_LEN]\n",
    "        doc_len=len(doc_words)\n",
    "\n",
    "        doc_vocab=list(set(doc_words))\n",
    "        doc_nodes=len(doc_vocab)\n",
    "\n",
    "        doc_len_list.append(doc_nodes)\n",
    "        vocab_set.update(doc_vocab)\n",
    "\n",
    "        doc_word_id_map={}\n",
    "        for j in range (doc_nodes):\n",
    "            doc_word_id_map[doc_vocab[j]]=j\n",
    "\n",
    "        #sliding windows\n",
    "        windows=[]\n",
    "        if doc_len<=window_size:\n",
    "            windows.append(doc_words)\n",
    "        else:\n",
    "            for j in range(doc_len-window_size+1):\n",
    "                window=doc_words[j:j+window_size]\n",
    "                windows.append(window)\n",
    "\n",
    "        word_pair_count={}\n",
    "        for window in windows:\n",
    "            for p in range(1,len(window)):\n",
    "                for q in range(0,p):\n",
    "                    word_p=window[p]\n",
    "                    word_p_id=word_id_map[word_p]\n",
    "                    word_q=window[q]\n",
    "                    word_q_id=word_id_map[word_q]\n",
    "                    if word_p_id == word_q_id:\n",
    "                        continue\n",
    "                    word_pair_key=(word_q_id,word_p_id)\n",
    "                    if word_pair_key in word_pair_count:\n",
    "                        word_pair_count[word_pair_key]+=1\n",
    "                    else:\n",
    "                        word_pair_count[word_pair_key]=1\n",
    "                    \n",
    "                    #bi-direction\n",
    "                    word_pair_key=(word_q_id,word_p_id)\n",
    "                    if word_pair_key in word_pair_count:\n",
    "                        word_pair_count[word_pair_key]+=1\n",
    "                    else:\n",
    "                        word_pair_count[word_pair_key]=1\n",
    "        \n",
    "        row=[]\n",
    "        col=[]\n",
    "        weight=[]\n",
    "        features=[]\n",
    "\n",
    "        for key in word_pair_count:\n",
    "            p=key[0]\n",
    "            q=key[1]\n",
    "            row.append(doc_word_id_map[vocab[p]])\n",
    "            col.append(doc_word_id_map[vocab[q]])\n",
    "            weight.append(word_pair_count[key] if weighted_graph else 1.)\n",
    "        adj=sp.csr_matrix((weight,(row,col)),shape=(doc_nodes, doc_nodes))\n",
    "\n",
    "        for k,v in sorted(doc_word_id_map.items(),key=lambda x:x[1]):\n",
    "            features.append(word_embeddings[k] if k in word_embeddings else oov[k])\n",
    "            x_adj.append(adj)\n",
    "            x_feature.append(features)\n",
    "    return x_adj,x_feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building graph for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11426/11426 [00:02<00:00, 5627.32it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"building graph for training\")\n",
    "\n",
    "x_adj,x_feature=build_graph(start=0, end=len(train),weighted_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_graph(x_adj, x_feature):\n",
    "#     for adj, features in zip(x_adj, x_feature):\n",
    "#         # Chuyển đổi ma trận sparse thành ma trận dense\n",
    "#         adj_dense = adj.toarray()\n",
    "        \n",
    "#         # Tạo đồ thị từ ma trận kề\n",
    "#         G = nx.Graph(adj_dense)  # Tạo đồ thị từ ma trận kề\n",
    "        \n",
    "#         pos = nx.spring_layout(G)  # Phân bố lại các nút\n",
    "\n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=1500, edge_color='black', linewidths=1,\n",
    "#                 font_size=10)\n",
    "\n",
    "#         # Thêm thông tin đặc trưng cho mỗi nút\n",
    "#         for i, node in enumerate(G.nodes()):\n",
    "#             feature = features[i]\n",
    "#             plt.text(pos[node][0], pos[node][1], str(feature), fontsize=8, ha='center', va='center')\n",
    "\n",
    "#         plt.title(\"Word Graph\", fontsize=15)\n",
    "#         plt.show()\n",
    "\n",
    "# # Sử dụng hàm draw_graph để vẽ đồ thị từ x_adj và x_feature\n",
    "# draw_graph(x_adj, x_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_test_word_overlap(train):\n",
    "    train_unique_words=[]\n",
    "\n",
    "    for text in train[\"sentence\"]:\n",
    "        train_unique_words.extencd(text.split())\n",
    "    train_unique_words=list(set(train_unique_words))\n",
    "\n",
    "    test_unique_words=[]\n",
    "\n",
    "    for text in test[\"sentence\"]:\n",
    "        test_unique_words.extend(text.split())\n",
    "    test_unique_words=list(set(test_unique_words))  \n",
    "\n",
    "    overlap=[x for x in test_unique_words if x in train_unique_words]\n",
    "    print(\"overlap:\", np.round(len(overlap)/len(test_unique_words),3))\n",
    "\n",
    "#maybe plot chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slide giáo_trình .</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nhiệt_tình giảng_dạy gũi sinh_viên .</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>đi học full chuyên .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>áp_dụng công_nghệ thông_thiết giảng_dạy .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thầy giảng tập ví_dụ lớp .</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11421</th>\n",
       "      <td>môn game học hai hài vô chuyên_nghiệp .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11422</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11423</th>\n",
       "      <td>giao tập .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11424</th>\n",
       "      <td>giáo_viên dạy nhiệt_tình .</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11425</th>\n",
       "      <td>gói gọn doubledot tận_tình trình_độ nhu_cầu_mô...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11426 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  sentiment  topic\n",
       "0                                     slide giáo_trình .          2      1\n",
       "1                   nhiệt_tình giảng_dạy gũi sinh_viên .          2      0\n",
       "2                                   đi học full chuyên .          0      1\n",
       "3              áp_dụng công_nghệ thông_thiết giảng_dạy .          0      0\n",
       "4                             thầy giảng tập ví_dụ lớp .          2      0\n",
       "...                                                  ...        ...    ...\n",
       "11421            môn game học hai hài vô chuyên_nghiệp .          0      1\n",
       "11422                                                             2      0\n",
       "11423                                         giao tập .          0      0\n",
       "11424                         giáo_viên dạy nhiệt_tình .          2      0\n",
       "11425  gói gọn doubledot tận_tình trình_độ nhu_cầu_mô...          2      0\n",
       "\n",
       "[11426 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic\n",
      "0    0.714348\n",
      "1    0.192913\n",
      "3    0.048994\n",
      "2    0.043745\n",
      "Name: proportion, dtype: float64\n",
      "topic\n",
      "0    0.715098\n",
      "1    0.192560\n",
      "3    0.049015\n",
      "2    0.043326\n",
      "Name: proportion, dtype: float64\n",
      "topic\n",
      "0    0.714661\n",
      "1    0.192560\n",
      "3    0.049453\n",
      "2    0.043326\n",
      "Name: proportion, dtype: float64\n",
      "topic\n",
      "0    0.714661\n",
      "1    0.192560\n",
      "3    0.049453\n",
      "2    0.043326\n",
      "Name: proportion, dtype: float64\n",
      "topic\n",
      "0    0.714661\n",
      "1    0.192560\n",
      "3    0.049015\n",
      "2    0.043764\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = args.n_folds,shuffle=True, random_state = 42)\n",
    "train['fold'] = -1\n",
    "\n",
    "for idx, (_, val_idx) in enumerate(skf.split(train, train['topic'])):\n",
    "    train.loc[val_idx, 'fold'] = idx\n",
    "\n",
    "for fold in range(args.n_folds):\n",
    "    print(train[train['fold']==fold]['topic'].value_counts(normalize = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset and model\n",
    "\n",
    "class GraphDataset(DGLDataset):\n",
    "    def __init__(self,x_adj, x_feature, topic=None):\n",
    "        self.adj=x_adj\n",
    "        self.node=x_feature\n",
    "        self.topic=topic\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.adj)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        adj_sci=self.adj[idx]\n",
    "        G=dgl.from_scipy(adj_sci)\n",
    "\n",
    "        G.ndata[\"feat\"]=torch.stack([torch.tensor(x,dtype=torch.float) for x in self.node[idx]])\n",
    "\n",
    "        if self.topic is not None:\n",
    "            label=self.topic[idx]\n",
    "            return G, torch.tensor(label,dtype=torch.long)\n",
    "    \n",
    "        return G\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(GCN,self).__init__()\n",
    "        self.conv1=GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2=GraphConv(hidden_dim, hidden_dim)\n",
    "        self.avgpooling=AvgPooling()\n",
    "        self.classify=nn.Linear(hidden_dim,n_classes)\n",
    "\n",
    "    def forward (self, g, h):\n",
    "        h=F.relu(self.conv1(g,h))\n",
    "        h=F.relu(self.conv2(g,h))\n",
    "        h=self.avgpooling(g,h)\n",
    "\n",
    "        return self.classify(h)\n",
    "\n",
    "class GATClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_heads, n_classes):\n",
    "        super(GATClassifier, self).__init__()\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.gat1 = GATConv(in_dim, hidden_dim, num_heads)\n",
    "        self.gat2 = GATConv(hidden_dim*num_heads, hidden_dim, 1)\n",
    "        self.avgpooling = AvgPooling()\n",
    "        self.drop = nn.Dropout(p = 0.3)\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # Apply graph convolution and activation.\n",
    "        bs = h.shape[0]\n",
    "        h = F.relu(self.gat1(g, h))\n",
    "        h = h.reshape(bs, -1)\n",
    "        h = F.relu(self.gat2(g, h))\n",
    "        h = h.reshape(bs, -1)\n",
    "        h = self.drop(h)\n",
    "        h = self.avgpooling(g, h)\n",
    "        \n",
    "        return self.classify(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold (args, adj_list, node_list,model, fold=0):\n",
    "    train_idx=list(train[train[\"fold\"]!=fold].index)\n",
    "    val_idx=list(train[train[\"fold\"]==fold].index)\n",
    "\n",
    "    print(\"train:\", len(train_idx))\n",
    "    print(\"val:\",len(val_idx))\n",
    "\n",
    "    n_classes=train[\"topic\"].nunique()\n",
    "\n",
    "    train_=train[train[\"fold\"]!=fold].reset_index(drop=True)\n",
    "    val=train[train[\"fold\"]==fold].reset_index(drop=True)\n",
    "\n",
    "    train_adj, val_adj=[adj_list[i] for i in train_idx],[adj_list[i] for i in val_idx]\n",
    "    train_node, val_node=[node_list[i] for i in train_idx],[node_list[i] for i in val_idx]\n",
    "    train_label, val_label= train_['topic'].values, val['topic'].values\n",
    "\n",
    "    train_dataset=GraphDataset(train_adj,train_node, train_label)\n",
    "    val_dataset=GraphDataset(val_adj,val_node, val_label)\n",
    "\n",
    "    train_loader=GraphDataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader=GraphDataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    criterion=CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=args.lr)\n",
    "    scheduler=None\n",
    "\n",
    "    best_val_mrr=0\n",
    "\n",
    "    loss=[]\n",
    "    f1=[]\n",
    "    auc=[]\n",
    "    mrr=[]\n",
    "\n",
    "    for i in tqdm(range(args.epochs)):\n",
    "        print(f\"Epoch{i+1} / {args.epochs}\")\n",
    "\n",
    "        train_loss, train_f1, train_auc, train_mrr=one_epoch(train_loader, model, criterion,optimizer, scheduler, n_classes)\n",
    "        val_loss, val_f1, val_auc, val_mrr=validate(val_loader, model, criterion, optimizer, scheduler, n_classes)\n",
    "\n",
    "        print(\"Train Loss:\", train_loss, \"Train F1:\", train_f1, \"Train AUC:\", train_auc, \"Train MRR:\", train_mrr)\n",
    "        print(\"Validation Loss:\", val_loss, \"Validation F1:\", val_f1, \"Validation AUC:\", val_auc, \"Validation MRR:\",val_mrr)\n",
    "\n",
    "        loss.append((train_loss, val_loss))\n",
    "        f1.append((train_f1, val_f1))\n",
    "        auc.append((train_auc, val_auc))\n",
    "        mrr.append((train_mrr, val_mrr))\n",
    "        \n",
    "        if val_mrr > best_val_mrr:\n",
    "            torch.save(model.state_dict(), f'fold-{fold}.pt')\n",
    "            best_val_mrr = val_mrr\n",
    "            \n",
    "    return {'loss': loss, 'f1': f1, 'auc': auc, 'mrr': mrr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(train_loader, model, criterion, optimizer, scheduler, n_classes):\n",
    "    train_loss= 0\n",
    "    train_f1=0\n",
    "    train_auc=0\n",
    "\n",
    "    labels=[]\n",
    "    logits=[]\n",
    "\n",
    "    total=len(train_loader)\n",
    "    model.train()\n",
    "\n",
    "    for i, (G, label) in tqdm(enumerate(train_loader), total=total):\n",
    "\n",
    "        G=dgl.add_self_loop(G)\n",
    "        h=G.ndata[\"feat\"].float()\n",
    "        logit=model(G,h)\n",
    "        \n",
    "        loss=criterion(logit, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        label_np=label.detach().cpu().numpy()\n",
    "        logit_np=logit.softmax(-1).detach().cpu().numpy()\n",
    "\n",
    "        train_loss+=loss.item()/total\n",
    "        train_f1+=sklearn.metrics.f1_score(label_np, logit_np.argmax(-1), average=\"micro\")/total\n",
    "\n",
    "        labels.append(label_np)\n",
    "        logits.append(logit_np)\n",
    "\n",
    "    labels=np.concatenate(labels)\n",
    "    logits=np.concatenate(logits)\n",
    "\n",
    "    one_hot_labels= np.zeros((len(labels), n_classes))\n",
    "    one_hot_labels[np.arange(len(labels)), labels] = 1.0\n",
    "    \n",
    "    train_auc = sklearn.metrics.roc_auc_score(labels, logits, multi_class = 'ovo', labels = np.array([int(i) for i in range(n_classes)]))\n",
    "    train_mrr = sklearn.metrics.label_ranking_average_precision_score(one_hot_labels, logits)\n",
    "    \n",
    "    return train_loss, train_f1, train_auc, train_mrr\n",
    "    \n",
    "\n",
    "def validate(val_loader, model, criterion, n_classes):\n",
    "    val_loss = 0\n",
    "    val_f1 = 0\n",
    "    val_auc = 0\n",
    "    \n",
    "    labels = []\n",
    "    logits = []\n",
    "    \n",
    "    total = len(val_loader)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (G, label) in tqdm(enumerate(val_loader), total = total):\n",
    "\n",
    "            h = G.ndata['feat'].float()\n",
    "            logit = model(G, h)\n",
    "            loss = criterion(logit, label)\n",
    "\n",
    "            label_numpy = label.detach().cpu().numpy()\n",
    "            logit_numpy = logit.softmax(-1).detach().cpu().numpy()\n",
    "\n",
    "            val_loss += loss.item()/total\n",
    "            val_f1 += sklearn.metrics.f1_score(label_numpy, logit_numpy.argmax(-1), average = 'micro')/total\n",
    "\n",
    "        \n",
    "            labels.append(label_numpy)\n",
    "            logits.append(logit_numpy)\n",
    "\n",
    "        labels = np.concatenate(labels)\n",
    "        logits = np.concatenate(logits)\n",
    "    \n",
    "        \n",
    "        one_hot_labels= np.zeros((len(labels), n_classes))\n",
    "        one_hot_labels[np.arange(len(labels)), labels] = 1.0\n",
    "        \n",
    "        val_auc = sklearn.metrics.roc_auc_score(labels, logits, multi_class = 'ovo', labels = np.array([int(i) for i in range(n_classes)]))\n",
    "        val_mrr = sklearn.metrics.label_ranking_average_precision_score(one_hot_labels, logits)\n",
    "        \n",
    "    \n",
    "    return val_loss, val_f1, val_auc, val_mrr    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
